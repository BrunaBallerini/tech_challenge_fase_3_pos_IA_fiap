{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Auv1P9ppWms"
      },
      "outputs": [],
      "source": [
        "# Tech Challenge - Fine-Tuning de Modelo (Llama 3 8B 4-bit via Unsloth)\n",
        "#\n",
        "# Este notebook implementa todo o fluxo solicitado:\n",
        "#\n",
        "# 1. Carregamento e limpeza do dataset (trn_teste.json -> filtrando linhas com content vazio).\n",
        "# 2. Geração de arquivo limpo: data_titles_contents_cleaned.jsonl.\n",
        "# 3. Conversão para formato de instrução: formatted_products_chat_data.json (instruction/input/output).\n",
        "# 4. Preparação dos prompts no formato Alpaca-like.\n",
        "# 5. Split treino / validação.\n",
        "# 6. Baseline (inferência antes do fine-tuning).\n",
        "# 7. Fine-tuning LoRA em 4-bit (Unsloth) do modelo unsloth/llama-3-8b-bnb-4bit.\n",
        "# 8. Avaliação pós-treino (gera outputs para validação).\n",
        "# 9. Métricas simples (ROUGE-1/2/L, BLEU opcional, overlap de tokens).\n",
        "# 10. Salvamento de adaptadores e modelo fundido.\n",
        "# 11. Função de busca de título (fuzzy) + geração de descrição (simulando pergunta do usuário).\n",
        "# 12. Pipeline interativo (opcional).\n",
        "# 13. Exportação de resultados / logs.\n",
        "#\n",
        "# Requisitos do desafio:\n",
        "# - Pergunta do usuário sobre título de produto → recuperar título mais similar → gerar descrição aprendida.\n",
        "# - Mostrar diferença antes e depois do fine-tune.\n",
        "# - Documentar parâmetros principais.\n",
        "#\n",
        "# Observação: Para resultados melhores aumente:\n",
        "# - num_train_epochs / max_steps\n",
        "# - Tamanho do conjunto de treinamento\n",
        "# - Qualidade da limpeza (remoção de duplicados / truncamento)\n",
        "\n",
        "# Instalação de dependências principais\n",
        "# Ajuste a linha do Unsloth se estiver em ambiente local sem GPU ou com CUDA diferente.\n",
        "# %pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# %pip install -q transformers datasets accelerate peft bitsandbytes trl rapidfuzz evaluate sacrebleu\n",
        "# %pip install -q bert-score\n",
        "\n",
        "# Install necessary libraries\n",
        "%pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install -q transformers datasets accelerate peft bitsandbytes trl rapidfuzz evaluate sacrebleu bert-score\n",
        "%pip install -q bert-score rouge_score\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import html\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from rapidfuzz import process, fuzz\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from evaluate import load as load_metric\n",
        "from bert_score import score as bert_scorer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Dispositivo detectado: {DEVICE}\")\n",
        "\n",
        "DATA_RAW_PATH = \"trn_teste1.json\"  # Ajuste se necessário (o arquivo bruto original)\n",
        "CLEAN_JSONL_PATH = \"data_titles_contents_cleaned.jsonl\"\n",
        "FORMAT_DATA_JSON = \"formatted_products_chat_data.json\"\n",
        "RESULTS_DIR = \"results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LOAD_IN_4BIT = True\n",
        "DTYPE = None  # deixar None para Unsloth decidir\n",
        "\n",
        "\n",
        "# Hiperparâmetros principais\n",
        "EPOCHS = 3           # Aumentado para 3 épocas completas\n",
        "LR = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 4\n",
        "WARMUP_STEPS = 10    # Aumentado ligeiramente para estabilizar o início\n",
        "MAX_STEPS = -1       # Alterado para -1 para treinar por épocas\n",
        "LOGGING_STEPS = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Configurações carregadas.\")\n",
        "\n",
        "### 2. Limpeza e Preparação do Dataset\n",
        "#\n",
        "# Nesta seção, realizamos as seguintes etapas:\n",
        "# 1.  **Limpeza do Arquivo Bruto**: Carregamos o `trn_teste1.json`, removemos registros com títulos ou conteúdos vazios/inválidos, eliminamos duplicatas e filtramos por comprimento para garantir a qualidade dos dados. O resultado é salvo em `data_titles_contents_cleaned.jsonl`.\n",
        "# 2.  **Formatação para Instrução**: Convertemos os dados limpos para um formato de instrução (pergunta/resposta), ideal para o fine-tuning. O modelo aprenderá a associar um `input` (título do produto) a um `output` (descrição). O resultado é salvo em `formatted_products_chat_data.json`.\n",
        "# 3.  **Criação do `Dataset` Hugging Face**: Carregamos os dados formatados em um objeto `Dataset` da biblioteca `datasets`, que é o padrão para treinamento no ecossistema Hugging Face.\n",
        "# 4.  **Divisão em Treino e Validação**: Separamos o dataset em um conjunto de treino (95%) e um de validação (5%) para que possamos avaliar o desempenho do modelo em dados que ele não viu durante o treinamento.\n",
        "\n",
        "# CÉLULA DE LIMPEZA DE DADOS (MELHORADA)\n",
        "def limpar_arquivo_raw_para_jsonl(entrada: str, saida: str, min_content_len=20, max_content_len=4096):\n",
        "    \"\"\"\n",
        "    Lê um arquivo JSONL, filtra, limpa e grava em um novo arquivo JSONL.\n",
        "    - Remove linhas com 'title' ou 'content' vazios.\n",
        "    - Decodifica entidades HTML (ex: &#8217; -> ').\n",
        "    - Filtra conteúdos com base no comprimento (`min_content_len` e `max_content_len`).\n",
        "    - Remove registros com 'title' duplicado, mantendo apenas a primeira ocorrência.\n",
        "    \"\"\"\n",
        "    linhas_lidas, linhas_escritas = 0, 0\n",
        "    titulos_vistos = set()  # Conjunto para rastrear títulos únicos\n",
        "\n",
        "    with open(entrada, \"r\", encoding=\"utf-8\") as fin, open(saida, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for linha in fin:\n",
        "            linhas_lidas += 1\n",
        "            try:\n",
        "                obj = json.loads(linha.strip())\n",
        "\n",
        "                # Limpeza e decodificação\n",
        "                title = html.unescape(str(obj.get(\"title\", \"\")).strip())\n",
        "                content = html.unescape(str(obj.get(\"content\", \"\")).strip())\n",
        "\n",
        "                # Validação: Título e conteúdo não vazios, conteúdo com tamanho mínimo e título não duplicado\n",
        "                if title and content and min_content_len <= len(content) <= max_content_len and title not in titulos_vistos:\n",
        "                    titulos_vistos.add(title)\n",
        "                    fout.write(json.dumps({\"title\": title, \"content\": content}, ensure_ascii=False) + \"\\n\")\n",
        "                    linhas_escritas += 1\n",
        "            except (json.JSONDecodeError, AttributeError):\n",
        "                pass  # Ignora linhas malformadas\n",
        "\n",
        "    print(f\"Limpeza concluída. Lidas: {linhas_lidas}, Válidas e Únicas: {linhas_escritas}\")\n",
        "    return linhas_escritas\n",
        "\n",
        "\n",
        "def carregar_registros_jsonl(caminho:str):\n",
        "    \"\"\"Carrega registros de um arquivo JSONL para uma lista de dicionários.\"\"\"\n",
        "    registros = []\n",
        "    with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, linha in enumerate(f, 1):\n",
        "            linha = linha.strip()\n",
        "            if not linha:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(linha)\n",
        "                title = html.unescape(obj.get(\"title\",\"\").strip())\n",
        "                content = html.unescape(obj.get(\"content\",\"\").strip())\n",
        "                if title and content:\n",
        "                    registros.append({\"title\": title, \"content\": content})\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "    return registros\n",
        "\n",
        "def converter_para_formato_instruction(registros, instrucao=\"DESCRIBE THIS PRODUCT\"):\n",
        "    \"\"\"Converte uma lista de registros para o formato de instrução (instruction/input/output).\"\"\"\n",
        "    return {\n",
        "        \"instruction\": [instrucao]*len(registros),\n",
        "        \"input\": [r[\"title\"] for r in registros],\n",
        "        \"output\": [r[\"content\"] for r in registros],\n",
        "    }\n",
        "\n",
        "def salvar_json(dados, caminho:str):\n",
        "    \"\"\"Salva um dicionário como um arquivo JSON formatado.\"\"\"\n",
        "    with open(caminho, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Arquivo salvo: {caminho}\")\n",
        "\n",
        "# Gera data_titles_contents_cleaned.jsonl a partir do arquivo bruto\n",
        "if not Path(DATA_RAW_PATH).exists():\n",
        "    raise FileNotFoundError(f\"Arquivo bruto não encontrado: {DATA_RAW_PATH}\")\n",
        "\n",
        "print(\"Limpeza e criação do JSONL...\")\n",
        "_ = limpar_arquivo_raw_para_jsonl(DATA_RAW_PATH, CLEAN_JSONL_PATH)\n",
        "\n",
        "# Amostra de linhas limpas\n",
        "with open(CLEAN_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in range(3):\n",
        "        print(f\"Exemplo linha limpa {i+1}: {f.readline().strip()}\")\n",
        "\n",
        "registros = carregar_registros_jsonl(CLEAN_JSONL_PATH)\n",
        "print(f\"Total de registros carregados: {len(registros)}\")\n",
        "\n",
        "dataset_chat = converter_para_formato_instruction(registros, \"DESCRIBE THIS PRODUCT\")\n",
        "salvar_json(dataset_chat, FORMAT_DATA_JSON)\n",
        "\n",
        "print(\"Amostra:\")\n",
        "for i in range(2):\n",
        "    print(f\"Instrução: {dataset_chat['instruction'][i]}\")\n",
        "    print(f\"Input: {dataset_chat['input'][i]}\")\n",
        "    print(f\"Output(len={len(dataset_chat['output'][i])}): {dataset_chat['output'][i][:120]}...\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "# Carrega o JSON (listas)\n",
        "hf_ds = Dataset.from_dict(dataset_chat)\n",
        "\n",
        "# Adiciona um índice para referência\n",
        "hf_ds = hf_ds.add_column(\"idx\", list(range(len(hf_ds))))\n",
        "\n",
        "# Split treino/val (ex: 95% treino / 5% validação)\n",
        "perc_valid = 0.05\n",
        "n_valid = max(1, int(len(hf_ds)*perc_valid))\n",
        "hf_ds = hf_ds.shuffle(seed=SEED)\n",
        "valid_ds = hf_ds.select(range(n_valid))\n",
        "train_ds = hf_ds.select(range(n_valid, len(hf_ds)))\n",
        "\n",
        "print(f\"Tamanho treino: {len(train_ds)} | validação: {len(valid_ds)}\")\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"validation\": valid_ds\n",
        "})\n",
        "\n",
        "print(dataset_dict)\n"
      ],
      "metadata": {
        "id": "4buQqOt8qtDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 3. Formatação do Prompt (Estilo Alpaca)\n",
        "#\n",
        "# Para que o modelo entenda a tarefa, formatamos cada exemplo de treino e validação usando um *template* de prompt. Escolhemos o formato \"Alpaca\", que é amplamente compatível com modelos de instrução.\n",
        "#\n",
        "# O template estrutura a informação da seguinte forma:\n",
        "# -   **Instruction**: A tarefa que o modelo deve executar (ex: \"DESCRIBE THIS PRODUCT\").\n",
        "# -   **Input**: O contexto específico para a tarefa (o título do produto).\n",
        "# -   **Response**: Onde o modelo deve gerar sua resposta (a descrição do produto).\n",
        "#\n",
        "# Essa formatação é crucial para o sucesso do fine-tuning.\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(example_batch):\n",
        "    instructions = example_batch[\"instruction\"]\n",
        "    inputs = example_batch[\"input\"]\n",
        "    outputs = example_batch[\"output\"]\n",
        "    texts = []\n",
        "    for inst, inp, outp in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(inst, inp, outp)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "formatted_train = dataset_dict[\"train\"].map(formatting_prompts_func, batched=True, num_proc=1)\n",
        "formatted_valid = dataset_dict[\"validation\"].map(formatting_prompts_func, batched=True, num_proc=1)\n",
        "\n",
        "print(\"Exemplo texto formatado:\\n\")\n",
        "print(formatted_train[0][\"text\"][:500])\n"
      ],
      "metadata": {
        "id": "mvLDf2rpq0eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 4. Carregamento do Modelo Base e Avaliação de Baseline\n",
        "#\n",
        "# Antes de treinar, precisamos de um ponto de partida (baseline) para medir o progresso.\n",
        "#\n",
        "# 1.  **Carregamento do Modelo**: Usamos `unsloth/llama-3-8b-bnb-4bit`, uma versão otimizada do Llama 3 8B que consome menos memória graças à quantização em 4-bit. A biblioteca `Unsloth` acelera significativamente o carregamento e o treinamento.\n",
        "# 2.  **Geração de Baseline**: Pegamos algumas amostras do conjunto de validação e pedimos ao modelo (ainda não treinado) para gerar descrições. Isso nos mostra como o modelo se comporta \"de fábrica\".\n",
        "# 3.  **Análise**: Como esperado, as descrições geradas pelo modelo base são genéricas, repetitivas ou simplesmente erradas, pois ele ainda não foi especializado na nossa tarefa. Salvamos esses resultados em `baseline_samples.csv` para comparação posterior.\n",
        "\n",
        "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype = DTYPE,\n",
        "    load_in_4bit = LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(\"Modelo base carregado.\")\n",
        "\n",
        "def gerar(model, tokenizer, instruction, input_text, max_new_tokens=128):\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
        "    decoded = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "    # Extrair somente a parte depois de ### Response: (heurística simples)\n",
        "    if \"### Response:\" in decoded:\n",
        "        decoded = decoded.split(\"### Response:\")[1].strip()\n",
        "    return decoded\n",
        "\n",
        "SAMPLES_BASELINE = min(5, len(formatted_valid))\n",
        "baseline_records = []\n",
        "for i in range(SAMPLES_BASELINE):\n",
        "    row = formatted_valid[i]\n",
        "    instruction = row[\"instruction\"]\n",
        "    input_title = row[\"input\"]\n",
        "    ref_output = row[\"output\"]\n",
        "    gen = gerar(model, tokenizer, instruction, input_title)\n",
        "    baseline_records.append({\n",
        "        \"idx\": row[\"idx\"],\n",
        "        \"title\": input_title,\n",
        "        \"ref\": ref_output,\n",
        "        \"gen_before\": gen\n",
        "    })\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_records)\n",
        "baseline_path = os.path.join(RESULTS_DIR, \"baseline_samples.csv\")\n",
        "baseline_df.to_csv(baseline_path, index=False)\n",
        "print(baseline_df)\n"
      ],
      "metadata": {
        "id": "vFO7UCsoq6ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 5. Configuração do Fine-Tuning com LoRA\n",
        "#\n",
        "# Em vez de treinar o modelo inteiro (o que seria computacionalmente caro), usamos a técnica **LoRA (Low-Rank Adaptation)**.\n",
        "#\n",
        "# -   **Como funciona?**: O LoRA congela os pesos originais do modelo e treina apenas pequenos \"adaptadores\" que são inseridos em camadas específicas (geralmente as de atenção).\n",
        "# -   **Vantagens**:\n",
        "#     -   **Eficiência**: Reduz drasticamente o número de parâmetros treináveis (de bilhões para alguns milhões).\n",
        "#     -   **Velocidade**: O treinamento é muito mais rápido.\n",
        "#     -   **Portabilidade**: Os adaptadores treinados são pequenos (alguns megabytes), facilitando o armazenamento e o compartilhamento.\n",
        "#\n",
        "# Configuramos o LoRA para ser aplicado às principais camadas de projeção do modelo (`q_proj`, `k_proj`, `v_proj`, etc.), que são cruciais para o aprendizado.\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Aumentado de 16 para 32\n",
        "    lora_alpha = 64, # Aumentado de 16 para 64 (dobro de r)\n",
        "    lora_dropout = 0.1, # Adicionado dropout de 10%\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = SEED,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"Modelo adaptado para LoRA.\")\n"
      ],
      "metadata": {
        "id": "WUPWSWO3rC-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 6. Treinamento do Modelo\n",
        "#\n",
        "# Com tudo configurado, iniciamos o treinamento usando o `SFTTrainer` (Supervised Fine-tuning Trainer) da biblioteca `trl`.\n",
        "#\n",
        "# -   **`TrainingArguments`**: Definimos os hiperparâmetros do treinamento, como:\n",
        "#     -   `per_device_train_batch_size` e `gradient_accumulation_steps`: Controlam o tamanho efetivo do batch, otimizando o uso de memória.\n",
        "#     -   `learning_rate`: A taxa de aprendizado.\n",
        "#     -   `max_steps`: O número total de passos de treinamento.\n",
        "#     -   `logging_steps`: A frequência com que o progresso do treino é registrado.\n",
        "# -   **`SFTTrainer`**: Orquestra todo o processo, alimentando o modelo com os dados formatados e aplicando as otimizações do LoRA e do Unsloth.\n",
        "#\n",
        "# Ao final, os adaptadores LoRA treinados são salvos no diretório `lora_adapters`.\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"outputs\",\n",
        "     num_train_epochs = EPOCHS,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    warmup_steps = WARMUP_STEPS,\n",
        "    max_steps = MAX_STEPS,\n",
        "    learning_rate = LR,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = LOGGING_STEPS,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = SEED,\n",
        "    # evaluation_strategy = \"no\",  # pode alterar para \"steps\" se quiser avaliar durante treino\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 200,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_train,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False,\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer configurado.\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "trainer.model.save_pretrained(\"lora_adapters\")\n",
        "tokenizer.save_pretrained(\"lora_adapters\")\n",
        "\n",
        "with open(os.path.join(RESULTS_DIR, \"training_log.txt\"), \"w\") as f:\n",
        "    f.write(str(train_result))\n",
        "\n",
        "print(\"Treinamento concluído.\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"DESCRIBE THIS PRODUCT\",\n",
        "        \"A Day in the Life of China\", # input\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "print(tokenizer.batch_decode(outputs))\n"
      ],
      "metadata": {
        "id": "TOQg8o3irnXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 7. Avaliação Pós-Treinamento e Comparação\n",
        "#\n",
        "# Agora, a parte mais importante: **o modelo melhorou?**\n",
        "#\n",
        "# 1.  **Preparação para Inferência**: Carregamos os adaptadores LoRA no modelo base. O Unsloth otimiza esse processo com `FastLanguageModel.for_inference()`.\n",
        "# 2.  **Geração em Batch**: Para avaliar o desempenho no conjunto de validação de forma eficiente, usamos um `pipeline` da `transformers` para gerar todas as descrições em *batch* (lotes). Isso é muito mais rápido do que gerar uma por uma.\n",
        "# 3.  **Coleta de Resultados**: Armazenamos as descrições geradas pelo modelo *fine-tuned* em `validation_generation_after_batch.csv`.\n",
        "# 4.  **Comparação Lado a Lado**: Criamos um DataFrame que mostra, para cada produto:\n",
        "#     -   O título (`title`).\n",
        "#     -   A descrição original (`ref`).\n",
        "#     -   A descrição gerada *antes* do treino (`gen_before`).\n",
        "#     -   A descrição gerada *depois* do treino (`gen_after`).\n",
        "#\n",
        "# Isso nos permite visualizar qualitativamente a melhoria. As respostas pós-treino devem ser muito mais precisas e contextuais.\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm # Ótimo para visualizar o progresso\n",
        "\n",
        "# Supondo que 'model' e 'tokenizer' já foram carregados e otimizados com Unsloth\n",
        "# FastLanguageModel.for_inference(model) # Você já fez isso, ótimo!\n",
        "\n",
        "# 1. Crie um pipeline de geração de texto\n",
        "#    - device=0 significa usar a primeira GPU.\n",
        "#    - O pipeline cuidará do batching automaticamente.\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # device=0 # Coloque o pipeline na GPU - REMOVIDO DEVIDO AO USO DE accelerate\n",
        ")\n",
        "\n",
        "# 2. Prepare todas as suas instruções e entradas (prompts) em uma lista\n",
        "#    Esta é a formatação que o modelo espera. Adapte se a sua for diferente.\n",
        "#    Exemplo de formatação para Alpaca/Llama:\n",
        "prompts = []\n",
        "for row in formatted_valid:\n",
        "    instruction = row[\"instruction\"]\n",
        "    input_title = row[\"input\"]\n",
        "\n",
        "    # Adapte este template de prompt para o que seu modelo foi treinado\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_title}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# 3. Execute a geração em batch\n",
        "#    - Defina um `batch_size` que caiba na sua memória. Comece com 8 ou 16 e ajuste.\n",
        "#    - `max_new_tokens` define o comprimento máximo da resposta gerada.\n",
        "#    - `pad_token_id` é importante para o batching funcionar corretamente.\n",
        "print(\"Iniciando a geração em batch...\")\n",
        "generated_outputs = text_generator(\n",
        "    prompts,\n",
        "    max_new_tokens=256,  # Ajuste conforme necessário\n",
        "    batch_size=8,        # <<-- PONTO CRÍTICO: Ajuste este valor!\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,      # Configurações de geração\n",
        "    top_p=0.9,\n",
        "    temperature=0.7,\n",
        ")\n",
        "print(\"Geração concluída.\")\n",
        "\n",
        "# 4. Processe os resultados\n",
        "eval_records = []\n",
        "for i, row in enumerate(formatted_valid):\n",
        "    # O pipeline retorna uma lista de listas de dicionários.\n",
        "    # Pegamos o texto gerado do primeiro (e único) resultado para cada prompt.\n",
        "    gen_after = generated_outputs[i][0]['generated_text']\n",
        "\n",
        "    # O resultado inclui o prompt. Vamos remover o prompt para ter apenas a resposta.\n",
        "    # Isso garante que 'gen_after' contenha apenas o texto novo.\n",
        "    gen_only = gen_after[len(prompts[i]):].strip()\n",
        "\n",
        "    eval_records.append({\n",
        "        \"idx\": row[\"idx\"],\n",
        "        \"title\": row[\"input\"],\n",
        "        \"ref\": row[\"output\"],\n",
        "        \"gen_after\": gen_only\n",
        "    })\n",
        "\n",
        "# 5. Salve os resultados como antes\n",
        "eval_df = pd.DataFrame(eval_records)\n",
        "eval_path = os.path.join(RESULTS_DIR, \"validation_generation_after_batch.csv\")\n",
        "eval_df.to_csv(eval_path, index=False)\n",
        "print(eval_df.head())\n"
      ],
      "metadata": {
        "id": "CumNpEjirrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "5gw6hIhrTosE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 8. Métricas de Avaliação Quantitativa\n",
        "#\n",
        "# Para medir a melhoria de forma objetiva, calculamos várias métricas padrão:\n",
        "#\n",
        "# -   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Mede a sobreposição de n-grams (sequências de palavras) entre o texto gerado e a referência. É bom para avaliar a qualidade do conteúdo.\n",
        "# -   **BLEU (Bilingual Evaluation Understudy)**: Mede a precisão, focando em quão similar é o texto gerado em relação à referência. É mais rígido que o ROUGE.\n",
        "# -   **BERTScore**: Em vez de olhar apenas para palavras, usa embeddings BERT para medir a similaridade semântica. É uma métrica mais robusta, pois entende o significado das palavras.\n",
        "# -   **Token Overlap**: Uma métrica simples que calcula a sobreposição de tokens (palavras únicas) entre o gerado e a referência.\n",
        "#\n",
        "# Comparamos os scores *antes* e *depois* do fine-tuning. Esperamos ver um aumento significativo em todas as métricas, confirmando que o modelo aprendeu a gerar descrições mais relevantes. Os resultados são salvos em `evaluation_metrics.json`.\n",
        "\n",
        "comparacao_df = pd.merge(baseline_df, eval_df[[\"idx\",\"gen_after\"]], on=\"idx\", how=\"left\")\n",
        "comparacao_path = os.path.join(RESULTS_DIR, \"comparacao_baseline_after.csv\")\n",
        "comparacao_df.to_csv(comparacao_path, index=False)\n",
        "print(comparacao_df)\n",
        "\n",
        "# CÁLCULO DE MÉTRICAS DE AVALIAÇÃO\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "sacrebleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    \"\"\"Calcula ROUGE, BLEU, BERTScore e Overlap de Tokens.\"\"\"\n",
        "\n",
        "    # Filtra linhas onde a geração pós-treino não é nula\n",
        "    df_valid = df.dropna(subset=['gen_after'])\n",
        "\n",
        "    # Extrai referências e predições\n",
        "    references = df_valid[\"ref\"].tolist()\n",
        "    predictions_before = df_valid[\"gen_before\"].tolist()\n",
        "    predictions_after = df_valid[\"gen_after\"].tolist()\n",
        "\n",
        "    # Garante que as predições não sejam listas vazias\n",
        "    if not predictions_after:\n",
        "        print(\"Nenhuma predição válida encontrada para calcular métricas.\")\n",
        "        return {}\n",
        "\n",
        "    # --- ROUGE ---\n",
        "    rouge_before = rouge_metric.compute(predictions=predictions_before, references=references)\n",
        "    rouge_after = rouge_metric.compute(predictions=predictions_after, references=references)\n",
        "\n",
        "    # --- BLEU ---\n",
        "    # Sacrebleu espera uma lista de listas de referências\n",
        "    bleu_references = [[ref] for ref in references]\n",
        "    bleu_before = sacrebleu_metric.compute(predictions=predictions_before, references=bleu_references)\n",
        "    bleu_after = sacrebleu_metric.compute(predictions=predictions_after, references=bleu_references)\n",
        "\n",
        "    # --- BERTScore ---\n",
        "    # Calcula BERTScore (P, R, F1)\n",
        "    P_before, R_before, F1_before = bert_scorer(predictions_before, references, lang=\"en\", model_type=\"bert-base-uncased\", device=DEVICE)\n",
        "    P_after, R_after, F1_after = bert_scorer(predictions_after, references, lang=\"en\", model_type=\"bert-base-uncased\", device=DEVICE)\n",
        "\n",
        "    bertscore_before = {\"precision\": P_before.mean().item(), \"recall\": R_before.mean().item(), \"f1\": F1_before.mean().item()}\n",
        "    bertscore_after = {\"precision\": P_after.mean().item(), \"recall\": R_after.mean().item(), \"f1\": F1_after.mean().item()}\n",
        "\n",
        "    # --- Token Overlap ---\n",
        "    def token_overlap(text1, text2):\n",
        "        tokens1 = set(text1.lower().split())\n",
        "        tokens2 = set(text2.lower().split())\n",
        "        intersection = len(tokens1.intersection(tokens2))\n",
        "        union = len(tokens1.union(tokens2))\n",
        "        return intersection / union if union > 0 else 0\n",
        "\n",
        "    overlap_before = np.mean([token_overlap(p, r) for p, r in zip(predictions_before, references)])\n",
        "    overlap_after = np.mean([token_overlap(p, r) for p, r in zip(predictions_after, references)])\n",
        "\n",
        "    metrics = {\n",
        "        \"before_finetune\": {\n",
        "            \"rouge\": rouge_before,\n",
        "            \"bleu\": bleu_before,\n",
        "            \"bert_score\": bertscore_before,\n",
        "            \"token_overlap\": overlap_before\n",
        "        },\n",
        "        \"after_finetune\": {\n",
        "            \"rouge\": rouge_after,\n",
        "            \"bleu\": bleu_after,\n",
        "            \"bert_score\": bertscore_after,\n",
        "            \"token_overlap\": overlap_after\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Calcular e exibir as métricas\n",
        "evaluation_metrics = calculate_metrics(comparacao_df)\n",
        "\n",
        "# Salvar métricas em um arquivo JSON\n",
        "metrics_path = os.path.join(RESULTS_DIR, \"evaluation_metrics.json\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "print(\"Métricas de avaliação calculadas e salvas.\")\n",
        "print(json.dumps(evaluation_metrics, indent=2))\n"
      ],
      "metadata": {
        "id": "4jjvgaeMr0sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 9. Simulação de Caso de Uso: Busca por Produto e Geração de Descrição\n",
        "#\n",
        "# Esta seção simula a aplicação prática do nosso sistema, conforme solicitado no desafio.\n",
        "#\n",
        "# 1.  **Consulta do Usuário**: Recebemos uma pergunta em linguagem natural sobre um produto (ex: \"tell me about the ballet tutu for girls\").\n",
        "# 2.  **Busca por Similaridade (Fuzzy Matching)**: Usamos a biblioteca `rapidfuzz` para encontrar o título de produto mais similar à consulta do usuário em nossa lista de produtos. Isso lida com erros de digitação, sinônimos ou formulações diferentes.\n",
        "# 3.  **Geração de Resposta**: Uma vez que o produto é identificado, passamos seu título exato para o modelo *fine-tuned*, que gera a descrição detalhada e precisa que aprendeu durante o treinamento.\n",
        "#\n",
        "# Este fluxo de trabalho demonstra um sistema de \"busca e resposta\" ponta a ponta, onde o usuário pode encontrar informações sobre um produto sem precisar saber seu nome exato.\n",
        "\n",
        "def generate_response(instruction, input_text):\n",
        "    # Format the input using the alpaca prompt\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        [prompt],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "\n",
        "    # Decode and return the response\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Extrai somente a parte depois de ### Response: (heurística simples)\n",
        "    response_start = response.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
        "    return response[response_start:].replace(tokenizer.eos_token, \"\").strip()\n",
        "\n",
        "# Example usage:\n",
        "instruction = \"DESCRIBE THIS PRODUCT\"\n",
        "input_text = \"Girls Ballet Tutu Neon Pink\"\n",
        "\n",
        "response = generate_response(instruction, input_text)\n",
        "print(response)\n",
        "\n",
        "# CARREGAR TÍTULOS PARA BUSCA\n",
        "# Garante que temos a lista de todos os títulos dos produtos para a busca por similaridade.\n",
        "all_titles = [r[\"title\"] for r in registros]\n",
        "print(f\"Total de {len(all_titles)} títulos carregados para a busca por similaridade.\")\n",
        "\n",
        "def find_and_describe_product(user_query: str, titles_list: list, similarity_threshold=80):\n",
        "    \"\"\"\n",
        "    Encontra o título mais similar a uma consulta de usuário e gera a descrição do produto.\n",
        "\n",
        "    1. Usa fuzzy matching para encontrar o título mais próximo na lista de produtos.\n",
        "    2. Se a similaridade for alta o suficiente, usa o modelo para gerar a descrição.\n",
        "    3. Retorna o título encontrado e a descrição gerada.\n",
        "    \"\"\"\n",
        "    # Encontra o melhor match para a consulta do usuário\n",
        "    # `process.extractOne` retorna uma tupla: (título, score, índice)\n",
        "    best_match = process.extractOne(user_query, titles_list, scorer=fuzz.WRatio)\n",
        "\n",
        "    if not best_match or best_match[1] < similarity_threshold:\n",
        "        return \"Produto não encontrado.\", f\"Nenhum produto com similaridade acima de {similarity_threshold}% encontrado para '{user_query}'.\", None\n",
        "\n",
        "    found_title, score, _ = best_match\n",
        "    print(f\"Consulta do usuário: '{user_query}'\")\n",
        "    print(f\"Título mais similar encontrado: '{found_title}' (Similaridade: {score:.2f}%)\")\n",
        "\n",
        "    # Gera a descrição para o título encontrado usando o modelo fine-tuned\n",
        "    print(\"\\nGerando descrição com o modelo...\")\n",
        "    generated_description = generate_response(\"DESCRIBE THIS PRODUCT\", found_title)\n",
        "\n",
        "    return found_title, generated_description, score\n",
        "\n",
        "# --- Exemplo de uso da função de busca e geração ---\n",
        "user_question = \"Can you tell me about the ballet tutu for girls?\"\n",
        "found_title, description, score = find_and_describe_product(user_question, all_titles)\n",
        "\n",
        "print(\"\\n--- RESULTADO FINAL ---\")\n",
        "print(f\"Produto Correspondente: {found_title}\")\n",
        "print(f\"Descrição Gerada:\\n{description}\")\n"
      ],
      "metadata": {
        "id": "Avi9qtmjr6mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 10. Salvando o Modelo para Produção (Opcional)\n",
        "#\n",
        "# Após o treinamento e a validação, o próximo passo seria preparar o modelo para ser usado em um ambiente de produção.\n",
        "#\n",
        "# -   **Mesclar Adaptadores**: Para simplificar a implantação, podemos mesclar os pesos do LoRA diretamente no modelo base. Isso cria um novo modelo que não precisa mais dos arquivos de adaptadores separados. O Unsloth facilita isso com `model.merge_and_unload()`.\n",
        "# -   **Salvar Modelo Mesclado**: O modelo completo e mesclado pode ser salvo usando `model.save_pretrained(\"merged_model\")`.\n",
        "# -   **Quantização GGUF**: Para máxima portabilidade e eficiência (especialmente para rodar em CPUs ou dispositivos com menos VRAM), podemos converter o modelo para o formato **GGUF**. Isso o torna compatível com ferramentas como `llama.cpp`.\n",
        "#\n",
        "# Esta etapa garante que o modelo possa ser facilmente integrado a aplicações como APIs, chatbots ou sistemas de busca.\n",
        "\n",
        "# (Opcional) Mesclar adaptadores e salvar modelo pronto para inferência standalone\n",
        "# Esta etapa é útil para criar um modelo único que não depende mais dos adaptadores LoRA.\n",
        "\n",
        "# 1. Mesclar os pesos do LoRA com o modelo base\n",
        "# O modelo agora se comporta como um modelo padrão da Hugging Face\n",
        "# model = model.merge_and_unload() # Descomente para mesclar\n",
        "\n",
        "# 2. Salvar o modelo mesclado (formato Hugging Face)\n",
        "# model.save_pretrained(\"llama3-8b-product-desc-merged\")\n",
        "# tokenizer.save_pretrained(\"llama3-8b-product-desc-merged\")\n",
        "\n",
        "# 3. (Avançado) Salvar no formato GGUF para inferência em CPU com llama.cpp\n",
        "# model.save_pretrained_gguf(\"llama3-8b-product-desc\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "print(\"Adaptadores salvos em ./lora_adapters.\")\n",
        "print(\"Para salvar o modelo mesclado ou em formato GGUF, descomente as linhas nesta célula.\")\n",
        "\n",
        "## Resumo Final\n",
        "#\n",
        "# Artefatos gerados:\n",
        "# - data_titles_contents_cleaned.jsonl (dados limpos)\n",
        "# - formatted_products_chat_data.json (instruction/input/output)\n",
        "# - baseline_samples.csv (amostras antes do treino)\n",
        "# - validation_generation_after.csv (gera pós-treino)\n",
        "# - comparacao_baseline_after.csv (comparativo)\n",
        "# - metrics.json (ROUGE, BLEU, overlap)\n",
        "# - lora_adapters/ (pesos LoRA)\n",
        "# - batch_queries_results.json (exemplos de perguntas)\n",
        "# - outputs/ (logs do Trainer)\n",
        "#\n",
        "# Próximos Passos Recomendados:\n",
        "# 1. Aumentar número de passos / épocas.\n",
        "# 2. Aplicar filtragem de descrições muito curtas/longas.\n",
        "# 3. Introduzir truncamento inteligente (tokenizer truncation).\n",
        "# 4. Adicionar métricas de similaridade semântica (BERTScore).\n",
        "# 5. Publicar adaptadores no HuggingFace Hub.\n",
        "# 6. Integrar a um endpoint (FastAPI / Gradio).\n",
        "#\n",
        "# Fim."
      ],
      "metadata": {
        "id": "QPgxrcQ3r_i4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
